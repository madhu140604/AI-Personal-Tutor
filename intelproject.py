# -*- coding: utf-8 -*-
"""intelproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sf4Tv0WkzheC6qGlz9K1nY1xHka_LSE_
"""

pip install faker

import random
import pandas as pd
from faker import Faker

fake = Faker()

# Defining correlated distributions
def get_iq():
    """Assign IQ based on a natural distribution, with fewer high-IQ students."""
    probability = random.random()

    if probability < 0.05:  # 5% chance of very high IQ
        return random.randint(126, 140)
    elif probability < 0.30:  # 25% chance of high IQ
        return random.randint(111, 125)
    else:  # 70% chance of moderate to low IQ
        return random.randint(85, 110)



def get_memory_power(iq):
    """Memory power correlates with IQ."""
    if iq >= 130:
        return random.randint(8, 10)
    elif iq >= 110:
        return random.randint(6, 9)
    elif iq >= 90:
        return random.randint(4, 7)
    else:
        return random.randint(2, 5)

def get_concentration(memory_power):
    """Concentration slightly follows memory power."""
    return max(1, min(10, memory_power + random.choice([-1, 0, 1])))

# Fixing the assessment score logic to ensure valid range
def get_assessment_score(iq):
    """Higher IQ students tend to score better."""
    base_score = iq - 40 if iq > 110 else iq - 50
    return max(0, min(100, base_score + random.randint(-10, 10)))  # Keeping scores within 0-100


def get_course_level(score):
    """Determine course level based on assessment score."""
    if score > 75:
        return 3
    elif score > 50:
        return 2
    else:
        return 1

def determine_learning_parameters(iq, memory_power, concentration, assessment_score):
    """Determine student learning parameters based on IQ, memory, concentration, and assessment score."""

    # Determine Student Level based on Assessment Score
    if assessment_score > 75:
        student_level = 3
    elif assessment_score > 50:
        student_level = 2
    else:
        student_level = 1

    # Determine Number of Examples based on IQ and Memory Power
    if iq > 125 or memory_power >= 9:
        num_examples = 3  # Fast learners need fewer examples
    elif iq > 110 or memory_power >= 7:
        num_examples = 5
    else:
        num_examples = 7  # Students with lower IQ/memory need more examples

    # Determine Break Interval (minutes before a break)
    if concentration >= 9:
        break_interval = 50
    elif concentration >= 7:
        break_interval = 40
    else:
        break_interval = 30

    # Determine Number of Questions in a Test (Fewer for High IQ students)
    if iq > 125 or assessment_score > 85:
        num_test_questions = 10
    elif iq > 110 or assessment_score > 65:
        num_test_questions = 12
    else:
        num_test_questions = 15

    # Determine Course Duration (in days)
    if student_level == 3:
        course_duration = 15  # Advanced students finish quickly
    elif student_level == 2:
        course_duration = 20
    else:
        course_duration = 25  # Beginners take longer

    return student_level, num_examples, break_interval, num_test_questions, course_duration

# Parameters
num_records = 1000

# Lists of possible values
earning_classes = ["Low", "Lower Middle", "Upper Middle", "High"]
occupations = ["Laborer", "Teacher", "Engineer", "Doctor", "Businessperson", "IT Professional", "Clerk", "Farmer"]
genders = ["M", "F"]
countries = ["USA", "India", "UK", "Canada"]
states = {"India": ["Tamil Nadu", "Maharashtra", "Delhi"], "USA": ["California", "Texas", "New York"],
          "UK": ["England", "Scotland"], "Canada": ["Ontario", "Quebec"]}
cities = {"Tamil Nadu": ["Chennai", "Coimbatore"], "Maharashtra": ["Mumbai", "Pune"],
          "Delhi": ["New Delhi"], "California": ["Los Angeles", "San Francisco"],
          "Texas": ["Houston", "Dallas"], "New York": ["New York City", "Buffalo"],
          "England": ["London", "Manchester"], "Scotland": ["Edinburgh", "Glasgow"],
          "Ontario": ["Toronto", "Ottawa"], "Quebec": ["Montreal", "Quebec City"]}

columns = ["Name", "Age", "Gender", "Country", "State", "City", "Parent Occupation",
           "Earning Class", "Course Level", "Assessment Score", "Time per Day",
           "IQ", "Memory Power", "Concentration","student_level", "num_examples",
           "break_interval", "num_test_questions", "course_duration","course_name",
           "material_name","material_level"]

data = []
for _ in range(num_records):
    name = fake.first_name()
    age = random.randint(8, 18)
    gender = random.choice(genders)
    country = random.choice(countries)
    state = random.choice(states[country])
    city = random.choice(cities[state])
    parent_occupation = random.choice(occupations)
    earning_class = random.choice(earning_classes)
    iq = get_iq()
    memory_power = get_memory_power(iq)
    concentration = get_concentration(memory_power)
    assessment_score = get_assessment_score(iq)
    course_level = get_course_level(assessment_score)
    time_per_day = random.randint(1, 6) if course_level == 1 else random.randint(2, 8)
    student_level, num_examples, break_interval, num_test_questions, course_duration=determine_learning_parameters(iq, memory_power, concentration, assessment_score)
    if(course_level==1):course_name="Basic_Python"
    elif(course_level==2):course_name="Intermediate_Python"
    else:course_name="Learn_Good_Python"
    material_name="Python_material"
    material_level=1
    data.append([name, age, gender, country, state, city, parent_occupation, earning_class,
                 course_level, assessment_score, time_per_day, iq, memory_power, concentration,student_level,
                 num_examples, break_interval, num_test_questions, course_duration,course_name,
                 material_name,material_level])

# Convert to DataFrame
df = pd.DataFrame(data, columns=columns)
df.head(30)  # Displaying first 30 records for review

df.info()

df.to_csv("ai_intern_data.csv", index=False)

"""# **import **"""

# prompt: Do EDA for the above data

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
df = pd.read_csv("ai_intern_data.csv")

# Display basic information
print(df.info())
print(df.describe())
# Check for missing values
print(df.isnull().sum())

# Explore numerical features
numerical_features = ['Age', 'IQ', 'Memory Power', 'Concentration', 'Assessment Score', 'Time per Day', 'student_level', 'num_examples', 'break_interval', 'num_test_questions', 'course_duration', 'material_level']
for col in numerical_features:
    plt.figure()
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()
# Explore categorical features
categorical_features = ['Gender', 'Country', 'State', 'City', 'Parent Occupation', 'Earning Class', 'Course Level','course_name','material_name']
for col in categorical_features:
    plt.figure()
    sns.countplot(y=col, data=df) # Using y for better readability
    plt.title(f'Counts of {col}')
    plt.show()

# Analyze relationships between features
sns.pairplot(df[numerical_features])
plt.show()
# Correlation matrix
correlation_matrix = df[numerical_features].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Numerical Features')
plt.show()
# Box plots to visualize the distribution of numerical features across different categories
for col in numerical_features:
  plt.figure()
  sns.boxplot(x='Course Level', y=col, data=df)
  plt.title(f"Boxplot of {col} across Course Levels")
  plt.show()
# Example: Relationship between IQ and Assessment Score
plt.figure()
sns.scatterplot(x='IQ', y='Assessment Score', data=df, hue='Course Level')
plt.title('Relationship between IQ and Assessment Score')
plt.show()

# ====================================
# FEATURE ENGINEERING & IMPORTANCE ANALYSIS
# ====================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

# Set visual style
sns.set(style="whitegrid")

# -------------------------------
# STEP 0: Encode categorical variables (if any)
# -------------------------------
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
if categorical_cols:
    print("Encoding categorical columns:", categorical_cols)
    le = LabelEncoder()
    for col in categorical_cols:
        df[col] = le.fit_transform(df[col])

# -------------------------------
# STEP 1: Define Inputs & Outputs
# -------------------------------
input_vars = ['Age', 'Assessment Score', 'Time per Day', 'IQ', 'Memory Power', 'Concentration']
output_vars = ['student_level', 'num_examples', 'break_interval', 'num_test_questions', 'course_duration']

feature_importance_all = {}

# -------------------------------
# STEP 2: Feature Importance for Each Output
# -------------------------------
for target in output_vars:
    X = df[input_vars]
    y = df[target]

    # Choose model based on number of unique values in output
    if y.nunique() <= 10:
        model = RandomForestClassifier(n_estimators=100, random_state=42)
    else:
        model = RandomForestRegressor(n_estimators=100, random_state=42)

    model.fit(X, y)
    importances = pd.Series(model.feature_importances_, index=input_vars).sort_values(ascending=False)
    feature_importance_all[target] = importances

    # Plot importance
    plt.figure(figsize=(8, 5))
    sns.barplot(x=importances.values, y=importances.index, palette="viridis")
    plt.title(f"Feature Importance for Predicting '{target}'")
    plt.xlabel("Importance")
    plt.ylabel("Input Feature")
    plt.tight_layout()
    plt.show()

# -------------------------------
# STEP 3: Average Feature Importance
# -------------------------------
aggregate_importance = pd.DataFrame(feature_importance_all).mean(axis=1).sort_values(ascending=False)
print("\nAverage Feature Importance Across All Output Variables:")
print(aggregate_importance)

# Plot average importance
plt.figure(figsize=(8, 5))
sns.barplot(x=aggregate_importance.values, y=aggregate_importance.index, palette="magma")
plt.title("Average Feature Importance Across All Outputs")
plt.xlabel("Average Importance")
plt.tight_layout()
plt.show()

# -------------------------------
# STEP 4: Drop Low-Importance Features (Optional)
# -------------------------------
threshold = 0.02
low_importance_features = aggregate_importance[aggregate_importance < threshold].index.tolist()

print("\nFeatures Considered for Dropping (Importance < 0.02):")
print(low_importance_features)
low_importance_features.remove('Time per Day')
low_importance_features.remove('Age')
# List of obviously irrelevant columns for ML prediction
#irrelevant_columns = ['Name', 'Gender', 'Country', 'State', 'City',
#                      'Parent Occupation', 'Material Name', 'Material Level']

# Drop them if they exist in the DataFrame
#df = df.drop(columns=[col for col in irrelevant_columns if col in df.columns])
#print("Remaining columns after dropping irrelevant ones:", df.columns.tolist())

# If any features fall below the threshold, drop them from df
if low_importance_features:
    df_cleaned = df.drop(columns=low_importance_features)
    print("Cleaned DataFrame shape after dropping low-importance features:", df_cleaned.shape)
else:
    df_cleaned = df.copy()
    print("No features were dropped. All features are important.")

# Optional: display cleaned dataset
df_cleaned.head()

### Influential Input Variables per Output Variable
### Influential Input Variables per Output Variable

#### 1. student_level
#- Assessment Score
#- IQ
#- Memory Power
#- Concentration

#### 2. num_examples
#- Concentration
#- Assessment Score
#- Memory Power

#### 3. break_interval
#- Concentration
#- Time per Day
#- Age

#### 4. num_test_questions
#- Assessment Score
#- IQ
#- Concentration

#### 5. course_duration
#- Time per Day
#- Assessment Score
#- Memory Power
#- IQ
#- Concentration

#### 2. num_examples
#- Concentration
#- Assessment Score
#- Memory Power

#### 3. break_interval
#- Concentration
#- Time per Day
#- Age

#### 4. num_test_questions
#- Assessment Score
#- IQ
#- Concentration

#### 5. course_duration
#- Time per Day
#- Assessment Score
#- Memory Power
#- IQ

!pip uninstall -y catboost
!pip install --no-cache-dir catboost



from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd

# ------------------------
# STEP 1: Feature influence weights from your analysis
# ------------------------
feature_importance_student_level = {
    'Assessment Score': 0.6717,
    'IQ': 0.2126,
    'Memory Power': 0.0492,
    'Time per Day': 0.0341,
    'Concentration': 0.0248,
    'Age': 0.0073
}

# Select top influencing features
features = list(feature_importance_student_level.keys())
weights = list(feature_importance_student_level.values())

# ------------------------
# STEP 2: Prepare weighted inputs
# ------------------------
X = df_cleaned[features].copy()

# Scale features by their importance (multiply each column by its weight)
for i, feature in enumerate(features):
    X[feature] *= weights[i]

y = df_cleaned['student_level'] - 1  # Adjust labels to 0, 1, 2 (as needed by XGBoost)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ------------------------
# STEP 3: Define models
# ------------------------
models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),
    "CatBoost": CatBoostClassifier(verbose=0, random_state=42),
    "MLP Classifier": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)
}

# ------------------------
# STEP 4: Evaluate models
# ------------------------
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    })

# ------------------------
# STEP 5: Show results
# ------------------------
results_df = pd.DataFrame(results)
print("\nðŸŽ¯ Performance with Feature Influence Scaling:")
print(results_df.sort_values(by="F1 Score", ascending=False))

rf_model_student_level=RandomForestClassifier()

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import pandas as pd

# ------------------------
# STEP 1: Feature importance (weights)
# ------------------------
feature_importance_num_examples = {
    'Memory Power': 0.4996,
    'IQ': 0.2237,
    'Concentration': 0.1419,
    'Assessment Score': 0.1182,
    'Age': 0.0087,
    'Time per Day': 0.0080
}

features = list(feature_importance_num_examples.keys())
weights = list(feature_importance_num_examples.values())

# ------------------------
# STEP 2: Weighted input data
# ------------------------
X = df_cleaned[features].copy()

for i, feature in enumerate(features):
    X[feature] *= weights[i]

y = df_cleaned['num_examples']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ------------------------
# STEP 3: Define models
# ------------------------
models = {
    "Random Forest Regressor": RandomForestRegressor(n_estimators=100, random_state=42),
    "XGBoost Regressor": XGBRegressor(n_estimators=100, random_state=42),
    "Linear Regression": LinearRegression()
}

# ------------------------
# STEP 4: Evaluate models
# ------------------------
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    f1_like = 2 * (1 / (1/mae + 1/mse))  # Harmonic mean of MAE and MSE

    results.append({
        "Model": name,
        "Accuracy (RÂ²)": r2,
        "Precision (MAE)": mae,
        "Recall (MSE)": mse,
        "F1-Like Score": f1_like
    })

# ------------------------
# STEP 5: Show results
# ------------------------
results_df = pd.DataFrame(results)
print("\nðŸ“Š Performance with Feature Importance Weighting (num_examples):")
print(results_df.sort_values(by="F1-Like Score", ascending=False))


xgb_model_num_examples = XGBRegressor()

from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pandas as pd
import numpy as np

# ------------------------
# STEP 1: Feature importance (weights)
# ------------------------
feature_importance_break_interval = {
    'Concentration': 0.6710,
    'Memory Power': 0.1795,
    'IQ': 0.0744,
    'Assessment Score': 0.0580,
    'Age': 0.0088,
    'Time per Day': 0.0083
}

features = list(feature_importance_break_interval.keys())
weights = list(feature_importance_break_interval.values())

# ------------------------
# STEP 2: Weighted input data
# ------------------------
X = df_cleaned[features].copy()
for i, feature in enumerate(features):
    X[feature] *= weights[i]

y = df_cleaned['break_interval']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ------------------------
# STEP 3: Define models
# ------------------------
models = {
    "Random Forest Regressor": RandomForestRegressor(n_estimators=100, random_state=42),
    "LightGBM Regressor": LGBMRegressor(n_estimators=100, random_state=42),
    "Ridge Regression": Ridge()
}

# ------------------------
# STEP 4: Evaluate models
# ------------------------

results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)

    # Safe harmonic mean (F1-like score)
    if mae == 0 or mse == 0:
        f1_like = 0
    else:
        f1_like = 2 * (1 / (1/mae + 1/mse))

    results.append({
        "Model": name,
        "Accuracy (RÂ²)": r2,
        "Precision (MAE)": mae,
        "Recall (MSE)": mse,
        "F1-Like Score": f1_like
    })

# ------------------------
# STEP 5: Show results
# ------------------------
results_df = pd.DataFrame(results)
print("\nðŸ“Š Performance with Feature Importance Weighting (break_interval):")
print(results_df.sort_values(by="F1-Like Score", ascending=False))

rf_model_break_interval = RandomForestRegressor()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pandas as pd

# Filter features by their importance for 'num_test_questions'
important_features = ['Assessment Score', 'IQ', 'Memory Power', 'Concentration', 'Age', 'Time per Day']
X = df_cleaned[important_features]
y = df_cleaned['num_test_questions']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Models to test
models = {
    "XGBoost Regressor": XGBRegressor(n_estimators=100, random_state=42),
    "Random Forest Regressor": RandomForestRegressor(n_estimators=100, random_state=42),
    "MLP Regressor": MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)
}

# Evaluate models
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)

    # Safe harmonic mean (F1-like score)
    if mae == 0 or mse == 0:
        f1_like = 0
    else:
        f1_like = 2 * (1 / (1/mae + 1/mse))

    results.append({
        "Model": name,
        "Accuracy (RÂ²)": r2,
        "Precision (MAE)": mae,
        "Recall (MSE)": mse,
        "F1-Like Score": f1_like
    })

# Return results as DataFrame
pd.DataFrame(results).sort_values(by="F1-Like Score", ascending=False)
xgb_model_num_test_questions = XGBRegressor()

from sklearn.linear_model import LinearRegression, Ridge
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pandas as pd

# Sample data for testing (replace with actual df_cleaned in your environment)
df_course = df_cleaned.copy()
features = ['Assessment Score', 'IQ', 'Memory Power', 'Time per Day', 'Concentration', 'Age']
weights = [0.6717, 0.2126, 0.0493, 0.0342, 0.0249, 0.0073]

# Apply weights to features
for i, feat in enumerate(features):
    df_course[feat] = df_course[feat] * weights[i]

X = df_course[features]
y = df_course['course_duration']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "LightGBM Regressor": LGBMRegressor(random_state=42),
    "CatBoost Regressor": CatBoostRegressor(verbose=0, random_state=42),
    "Linear Regression": LinearRegression()
}

results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    f1_like = 2 * (1 / (1/mae + 1/mse)) if (mae != 0 and mse != 0) else 0

    results.append({
        "Model": name,
        "Accuracy (RÂ²)": round(r2, 4),
        "Precision (MAE)": round(mae, 4),
        "Recall (MSE)": round(mse, 4),
        "F1-like Score": round(f1_like, 4)
    })

results_df = pd.DataFrame(results).sort_values(by="F1-like Score", ascending=False)
results_df
lgb_model_course_duration = LGBMRegressor()

from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

# Define features and target for course_name prediction
features_course_name = ['Assessment Score', 'IQ', 'Memory Power', 'Time per Day', 'Concentration', 'Age']
X = df_cleaned[features_course_name]
y = df_cleaned['course_name']

# Encode course_name if it's categorical
if y.dtype == 'object':
    y = LabelEncoder().fit_transform(y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models
models_course_name = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "XGBoost Classifier": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# Evaluate models
results_course_name = []
for name, model in models_course_name.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    results_course_name.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='macro', zero_division=0),
        'Recall': recall_score(y_test, y_pred, average='macro', zero_division=0),
        'F1 Score': f1_score(y_test, y_pred, average='macro', zero_division=0)
    })

pd.DataFrame(results_course_name).sort_values(by="F1 Score", ascending=False)

from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from lightgbm import LGBMRegressor

# Features used for all regressors
input_features = [
    'Time per Day', 'Concentration', 'Age',
    'Assessment Score', 'IQ', 'Memory Power',
]

# 1. Train best model for each output

xgb_model_num_examples.fit(df[input_features], df['num_examples'])


rf_model_break_interval.fit(df[input_features], df['break_interval'])


xgb_model_num_test_questions.fit(df[input_features], df['num_test_questions'])


lgb_model_course_duration.fit(df[input_features], df['course_duration'])

# 2. Predict using trained models
df['num_examples_pred'] = xgb_model_num_examples.predict(df[input_features])
df['break_interval_pred'] = rf_model_break_interval.predict(df[input_features])
df['num_test_questions_pred'] = xgb_model_num_test_questions.predict(df[input_features])
df['course_duration_pred'] = lgb_model_course_duration.predict(df[input_features])

# 3. Prepare input for classification model
X = df[[
    'Assessment Score', 'IQ', 'Memory Power', 'Time per Day', 'Concentration', 'Age',
    'num_examples_pred', 'break_interval_pred', 'num_test_questions_pred', 'course_duration_pred'
]]
y = df['student_level']

# 4. Train Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X, y)

# 5. Show Feature Importance
importances = rf_classifier.feature_importances_
for name, val in zip(X.columns, importances):
    print(f"{name}: {val:.4f}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# ðŸ”¹ Use correct feature order for each model
features_num_examples = ['Time per Day', 'Concentration', 'Age', 'Assessment Score', 'IQ', 'Memory Power']
features_break_interval = features_num_examples
features_num_test_questions = features_num_examples
features_course_duration = features_num_examples

# ðŸ”¹ Predict intermediate outputs
df_cleaned['num_examples_pred'] = xgb_model_num_examples.predict(df_cleaned[features_num_examples])
df_cleaned['break_interval_pred'] = rf_model_break_interval.predict(df_cleaned[features_break_interval])
df_cleaned['num_test_questions_pred'] = xgb_model_num_test_questions.predict(df_cleaned[features_num_test_questions])
df_cleaned['course_duration_pred'] = lgb_model_course_duration.predict(df_cleaned[features_course_duration])


# ðŸ”¹ Step 3: Feature importances for student_level (based on your values)
feature_importance_student_level = {
    'Assessment Score': 0.3929,
    'IQ': 0.0846,
    'Memory Power': 0.0097,
    'Time per Day': 0.0081,
    'Concentration': 0.0032,
    'Age': 0.0006,
    'num_examples_pred': 0.0314,
    'break_interval_pred': 0.0020,
    'num_test_questions_pred': 0.0593,
    'course_duration_pred': 0.4084
}

# ðŸ”¹ Step 4: Weighted input preparation
student_level_features = list(feature_importance_student_level.keys())
X = df_cleaned[student_level_features].copy()

for feature, weight in feature_importance_student_level.items():
    X[feature] *= weight

y = df_cleaned['student_level']

# ðŸ”¹ Step 5: Train model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rf_student_level = RandomForestRegressor(n_estimators=100, random_state=42)
rf_student_level.fit(X_train, y_train)

# ðŸ”¹ Step 6: Evaluate
y_pred = rf_student_level.predict(X_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Avoid division by zero
if mae == 0 or mse == 0:
    f1_like = 0
else:
    f1_like = 2 * (1 / (1/mae + 1/mse))  # Harmonic mean of MAE and MSE


print("\nðŸ“Š Performance of RandomForest for student_level (with weighted features):")
print(f"âœ… Accuracy (RÂ²): {r2:.4f}")
print(f"âœ… Precision (MAE): {mae:.4f}")
print(f"âœ… Recall (MSE): {mse:.4f}")
print(f"âœ… F1-Like Score: {f1_like:.4f}")

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# ðŸ”¹ Step 1: Prepare the dataset
input_cols = ['Assessment Score', 'IQ', 'Memory Power', 'Time per Day', 'Concentration', 'Age']

# Train student_level model
X = df_cleaned[input_cols]
rf_model_student_level = RandomForestRegressor().fit(X, df_cleaned['student_level'])

# Train other models
xgb_model_num_examples = XGBRegressor().fit(X, df_cleaned['num_examples'])
rf_model_break_interval = RandomForestRegressor().fit(X, df_cleaned['break_interval'])
xgb_model_num_test_questions = XGBRegressor().fit(X, df_cleaned['num_test_questions'])
rf_model_course_duration = RandomForestRegressor().fit(X, df_cleaned['course_duration'])

# ---------------------------
# 2. Take user input
# ---------------------------
name = input("Enter Name: ")
age = int(input("Enter Age (8-18): "))
gender = input("Enter Gender (M/F): ")
country = input("Enter Country (US/UK/Canada/India): ")
state = input("Enter State: ")
city = input("Enter City: ")
parent_occupation = input("Enter Parent Occupation: ")
earning_class = input("Enter Earning Class (Low, Lower Middle, Upper Middle, High): ")
assessment_score = float(input("Enter Assessment Score (0-100): "))
time_per_day = float(input("Enter Time per Day (hours): "))
iq = int(input("Enter IQ (85-140): "))
memory_power = float(input("Enter Memory Power (2-10): "))
concentration = float(input("Enter Concentration (1-10): "))

# ---------------------------
# 3. Handle low Assessment Score
# ---------------------------
if assessment_score < 35:
    print("\nðŸš« Not passed to next level.")

    input_data = pd.DataFrame([{
        'Assessment Score': assessment_score,
        'IQ': iq,
        'Memory Power': memory_power,
        'Time per Day': time_per_day,
        'Concentration': concentration,
        'Age': age
    }])

    student_level = int(rf_model_student_level.predict(input_data)[0])
    course_level = student_level

    num_examples = int(xgb_model_num_examples.predict(input_data)[0])
    break_interval = int(rf_model_break_interval.predict(input_data)[0])
    num_test_questions = int(xgb_model_num_test_questions.predict(input_data)[0])
    course_duration = int(rf_model_course_duration.predict(input_data)[0])

    course_name = {
        1: "Basic_Python",
        2: "Intermediate_Python",
        3: "Learn_Good_Python"
    }.get(course_level, "Basic_Python")

    material_name = "Python_material"
    material_level = 1

    # ---------------------------
    # 4. Output all results
    # ---------------------------
    print("\nðŸŽ“ Prediction Results:")
    print(f"Name: {name}")
    print(f"Student Level: {student_level}")
    print(f"Course Level: {course_level}")
    print(f"Course Name: {course_name}")
    print(f"Material Name: {material_name}")
    print(f"Material Level: {material_level}")
    print(f"Number of Examples: {num_examples}")
    print(f"Break Interval (minutes): {break_interval}")
    print(f"Number of Test Questions: {num_test_questions}")
    print(f"Course Duration (days): {course_duration}")

